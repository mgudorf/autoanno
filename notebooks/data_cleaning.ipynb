{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "promising-quantum",
   "metadata": {},
   "source": [
    "# AutoAnno\n",
    "\n",
    "<a id='toc'></a>\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "## [1. Proposal](#proposal)\n",
    "## [2. Data Description](#data)\n",
    "## [3. Data Cleaning](#cleaning)\n",
    "## [4. Feature Engineering](#features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exclusive-briefs",
   "metadata": {},
   "source": [
    "<a id='proposal'></a>\n",
    "\n",
    "# Proposal\n",
    "\n",
    "[**Return to table of contents** ](#toc) \n",
    "\n",
    "A project that searches for a business solution to the automated annotation of big data.\n",
    "\n",
    "## Why?\n",
    "\n",
    "As a networking exercise I have interviewed around 40 or so data scientists and quantitative finance analysts. Natural Language Processing\n",
    "seems to be immensely popular in the current meta of data science. In all of the conversations, the one constant is the acceptance\n",
    "of annotation of text based data being a \"necessary evil\"; for supervised learning and modeling accurate labels are required. \n",
    "\n",
    "There are many companies dedicated to using ML and human resources to annotate data; therefore, doing this accurately and\n",
    "efficiently than humans can do on their own is a great business opportunity.\n",
    "\n",
    "In my opinion this investigation is bounded only by creativity; the following are some preliminary ideas. \n",
    "\n",
    "    1. Topological Data Analysis applied to embeddings of text data.\n",
    "    2. Convolutional neural networks applied to text data converted into images.\n",
    "    3. Using graph neural networks or just network graphs to better capture the context of the text data\n",
    "   \n",
    "## First attempt\n",
    "\n",
    "The first trial method is going to use topological data analysis to hopefully detect clusters in embedding text data,\n",
    "each cluster corresponding to an annotation label. Topological data analysis comes in a variety of flavors.\n",
    "The avenues I will investigate are the ToMATo algorithm and using a Vietoris-Rips complex on the embedding data.\n",
    "\n",
    "What does this jargon mean? Vietoris-rips, crudely speaking, is a name for an algorithm.\n",
    "This algorithm detects structures (topologically, holes) by a filtration method. \n",
    "\n",
    "This method centers an imaginary sphere at each data point. The radius of these spheres is uniformly increased (they are identical in size);\n",
    "whenever two spheres overlap, a simplex (line segment) is drawn connecting the centers (each data point). When three spheres overlap,\n",
    "the 2-simplex (triangle) that is defined by the three connecting line segments is \"filled in\", this pattern continues for more overlaps\n",
    "resulting in tetrahedrons, etc. See the image below for a 2-d example.\n",
    "\n",
    "![Rips complex](./rips.png)\n",
    "\n",
    "Clearly, as the radius of the spheres becomes as large as the \"diameter\" of the data points, all points are connected. \n",
    "The useful information comes from tracking the \"birth\" and \"death\" of the different shapes. \"birth\" and \"death\" correspond\n",
    "to creation of new simplicies (i.e. when spheres overlap), \"death\" corresponds to the merging of disjoint pieces; when they are connected\n",
    "by a new simplex. The births and deaths of simplicies of different dimension are tracked; the longer lived the components are, the\n",
    "more indicative they are of structure. \n",
    "\n",
    "# Preliminary project statement\n",
    "\n",
    "## First stage : topic annotation\n",
    "\n",
    "Breaking down each conversation into its smallest annotated components is the end goal, however this amounts to trying\n",
    "to detect small clusters; first, let's see if topic can be annotated. Towards this end, there are 6 `json` files, one for each topic:\n",
    "\n",
    "- flights.json\n",
    "- food-ordering.json\n",
    "- hotels.json\n",
    "- movies.json\n",
    "- music.json\n",
    "- restaurant-search.json\n",
    "- sports.json\n",
    "\n",
    "\n",
    "The first step is to get the data into a format that is compatible with various word embedding algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finished-macedonia",
   "metadata": {},
   "source": [
    "<a id='data'></a>\n",
    "\n",
    "# Data preview\n",
    "\n",
    "[**Return to table of contents** ](#toc) \n",
    "\n",
    "This notebook is dedicated to cleaning and exploring data from the Taskmaster dataset, namely, its second component `TM-2-2020`\n",
    "First, let us look at the README so we can figure out the best way to import it. \n",
    "\n",
    "### Original Taskmaster README\n",
    "\n",
    "**NUMBERS**\n",
    "The Taskmaster-2 dataset consists of 17,289 dialogs in the seven domains below. Dialogs for each domain can be found in the seven json files located in this directory's \"data\" folder, i.e. Taskmaster/TM-2-2-20/data/.\n",
    "* restaurants (3276)\n",
    "* food ordering (1050)\n",
    "* movies (3047)\n",
    "* hotels (2355)\n",
    "* flights (2481)\n",
    "* music (1602)\n",
    "* sports (3478)\n",
    "\n",
    "**STRUCTURE**\n",
    "Each conversation in the data file has the following structure:\n",
    "* __conversation_id:__ A universally unique identifier with the prefix 'dlg-'. The ID has no meaning.\n",
    "* __utterances:__ An array of utterances that make up the conversation.\n",
    "* __instruction_id:__ A reference to the file(s) containing the user (and, if applicable, agent) instructions for this conversation.\n",
    "\n",
    "Each utterance has the following fields:\n",
    "* __index:__ A 0-based index indicating the order of the utterances in the conversation.\n",
    "* __speaker:__ Either USER or ASSISTANT, indicating which role generated this utterance.\n",
    "* __text:__ The raw text of the utterance. 'ASSISTANT' turns are originally written (then played to the user via TTS) and 'USER' turns are transcribed from the spoken recordings of crowdsourced workers.\n",
    "* __segments:__ An array of various text spans with semantic annotations.\n",
    "\n",
    "Each segment has the following fields:\n",
    "* __start_index:__ The position of the start of the annotation in the utterance text.\n",
    "* __end_index:__ The position of the end of the annotation in the utterance text.\n",
    "* __text:__ The raw text that has been annotated.\n",
    "* __annotations:__ An array of annotation details for this segment.\n",
    "\n",
    "Each annotation has a single field:\n",
    "* __name:__ The annotation name.\n",
    "\n",
    "## Data Summary\n",
    "\n",
    "The text-based data is split between files, in a nested dict structure, the highest level contains tags by ID, and topic instructions.\n",
    "The next level are conversations, which contain the sentences or \"utterances\" as separate dicts. For each utterance there is another\n",
    "dictionary of \"segments\" which is everything in the annotation except the label. If an utterance is not annotized then this may be empty. \n",
    "If the annotation segments exist, then they are included as a dict with starting and stopping positions, w.r.t. string elements, the\n",
    "raw text, and the annotation label; this is included as another dict but it only consists of a key \"name\" and the label; and so is currently redundant\n",
    "for my purposes. \n",
    "\n",
    "I simply want the raw text data, in a format that permits exploration and modeling, and so I will either put each sentence in a list wherein the\n",
    "elements are words, or lists of conversations, where elements are words but the list contains the entire paragraph. As I am treating this\n",
    "as an unsupervised learning project, the annotation data is only used for evaluation, NOT training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "endangered-studio",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bound-enough",
   "metadata": {},
   "source": [
    "In other words, we have nested dictionaries. Building up from the lower level, let's look at different samples of keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "martial-antenna",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = {}\n",
    "# get the .json filenames\n",
    "for jsfiles in glob.glob('./Taskmaster/TM-2-2020/data/*.json'):\n",
    "    with open(jsfiles, 'r') as f:\n",
    "        # import as a list of nested dicts.\n",
    "        contents = json.load(f)\n",
    "        # for reference by label use a dict.\n",
    "        documents[os.path.basename(jsfiles).split('.')[0]] = contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clinical-cambodia",
   "metadata": {},
   "source": [
    "The original document labels the conversations by ID and by the instructions they are based off of. Only need the text\n",
    "for now; extract the relevant data from this dictionary level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "valuable-start",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now, tagging by conversation ID / \n",
    "all_topics =[[each_conversation['utterances'] for each_conversation in data]\n",
    "                          for each_topic, data in documents.items()]\n",
    "flight_conversations = all_topics[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-johnson",
   "metadata": {},
   "source": [
    "Each conversation is comprised of a sequence of utterances. Let's look at a conversation line by line to get an idea\n",
    "as to the state of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "opposite-dollar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Hello. I'd like to find a round trip commercial airline flight from San Francisco to Denver.\n",
      "1: Hello, how can I help you?\n",
      "2: San Francisco to Denver, got it.\n",
      "3: You're really on top of things. I like that.\n",
      "4: So what days are you looking to fly?\n",
      "5: Hey, what else can you say?\n",
      "6: I'm looking to fly out sometime today, the earliest time today, and I'll be returning in 4 days.\n",
      "7: So, I would like to fly out sometime tonight and fly back in the evening in 4 days. From I'm looking to go to Denver. I'm flying out of San Francisco.\n",
      "8: That sounds good, where you looking to go?\n",
      "9: That's right okay we have prices starting at $337.\n",
      "10: That sounds very good. I just have two preferences. I want a nonstop flight.\n",
      "11: And I'd like to get an aisle seat.\n",
      "12: Okay, Non-Stop and if I heard you correctly did you say you wanted to leave as early as possible and also Nile C.\n",
      "13: Yes.\n",
      "14: Okay, you got it so it looks like United Airlines leaves at 9:20 p.m. that is nonstop the flight duration is 2 hours and 28 minutes and is priced at $337.\n",
      "15: That sounds very good.\n",
      "16: Perfect.\n",
      "17: And what time will I be I would like an evening flight on the return flight in 4 days.\n",
      "18: Does that include the return flight?\n",
      "19: How does 5:30 a.m. work?\n",
      "20: Well, the evening works better.\n",
      "21: Okay, got it.\n",
      "22: 6:55 p.m.\n",
      "23: That works. That's very good.\n",
      "24: Alright, perfect so that is United Airlines as well.\n",
      "25: Excellent. Thank you so much, assistant.\n",
      "26: Pleasure all mine. Enjoy your trip to Denver, have a great night.\n",
      "27: Your welcome.\n",
      "28: I think I'm all set. Thank you so much.\n"
     ]
    }
   ],
   "source": [
    "for i, line in enumerate(flight_conversations[0]):\n",
    "    print(f\"{str(i)}: {line['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-southwest",
   "metadata": {},
   "source": [
    "While currently not going to be used, let's get an idea as to how specific the labels are, for future awareness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fossil-browser",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unique keys in the annotation dict are: \n",
      "{'name'}\n",
      "\n",
      "A subset of unique values in the annotation dict are: \n",
      "\n",
      "flight3_detail.to\n",
      "flight2_detail.date\n",
      "flight2_detail.flight_number\n",
      "flight1_detail.to\n",
      "flight4_detail.to\n",
      "flight1_detail.flight_number\n",
      "flight_search.seating_class\n",
      "flight1_detail.from.time\n",
      "flight_search.origin\n",
      "flight2_detail.to.time\n"
     ]
    }
   ],
   "source": [
    "label_set = set()\n",
    "label_val = set()\n",
    "for utterances_list in flight_conversations:\n",
    "    for utterances in utterances_list:\n",
    "        for segments in utterances.get('segments', ()):\n",
    "            for anno in segments['annotations']:\n",
    "                if isinstance(anno, dict):\n",
    "                    for name, target_label in anno.items():\n",
    "                        label_set.add(name)\n",
    "                        label_val.add(target_label)\n",
    "print(f\"The unique keys in the annotation dict are: {chr(10)}{label_set}{chr(10)}\")\n",
    "print(f\"A subset of unique values in the annotation dict are: {chr(10)+chr(10)}{chr(10).join(list(label_val)[:10])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advisory-artwork",
   "metadata": {},
   "source": [
    "<a id='cleaning'></a>\n",
    "\n",
    "# Data Cleaning and Wrangling.\n",
    "\n",
    "[**Return to table of contents** ](#toc) \n",
    "\n",
    "There are a number of important cleaning steps in order to get the text-based data ready for embedding,\n",
    "I will describe the transformations in detail next. As a manner of convention; the methods and functions\n",
    "sometimes introduce seemingly errant whitespace, however, it is much easier to insert whitespace and remove it later\n",
    "than try to guess where strings have been unduely concatenated. Any and all mappings use this liberally until\n",
    "the last mapping, which, as you may have guessed, shrinks all remaining white spaces to single instances.\n",
    "\n",
    "#### Example conversation\n",
    "\n",
    "First, print out line by line one of the conversations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-casino",
   "metadata": {},
   "source": [
    "#### 1. Numbers\n",
    "\n",
    "I want to replace numeric digits with their english based representations. While infrequent, I think numbers can be quite important in establishing context. for example, scores of sports games seem to be transcribed #-#. Times for flights, #:##, and so on. However, as can be seen in these two examples, they are often utilized in conjunction with other symbols, which will still be replaced due to a lack of convention between conversations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competent-blocking",
   "metadata": {},
   "source": [
    "To convert from numeric to word representations of the numbers, use the package ``inflect``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "automated-lighter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inflect\n",
    "\n",
    "p = inflect.engine()\n",
    "n2wfunc = p.number_to_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "architectural-reply",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_to_word_wrapper(val):\n",
    "    if isinstance(val, str) and (ord(val)>= 48) and (ord(val) <= 57):\n",
    "        buffer = ' '\n",
    "        return buffer + p.number_to_words(val) + buffer\n",
    "    else:\n",
    "        return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "extended-angle",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_text = []\n",
    "for each_topic in all_topics:\n",
    "    conversation_corpus = []\n",
    "    for each_conversation in each_topic:\n",
    "        utterance_corpus = []\n",
    "        for each_utterance in each_conversation:\n",
    "            utterance_corpus.append(''.join(list(map(num_to_word_wrapper, each_utterance['text']))))\n",
    "        conversation_corpus.append(utterance_corpus)\n",
    "    corpus_text.append(conversation_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "processed-commons",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How can I help you?',\n",
       " 'I was wondering, could you tell me how Real Salt Lake is doing in the standings?',\n",
       " 'This will just be in a second, please wait.',\n",
       " 'Okay.',\n",
       " 'Real Salt Lake is currently in  eight th Place in the Western Conference. They are  one  zero  games back from FC Dallas.',\n",
       " 'Okay. Did they play today?',\n",
       " 'Please wait for a second.',\n",
       " 'No, they do not play today. They do play tomorrow at  five  p.m against Columbus.',\n",
       " 'Okay, did they play last Saturday?',\n",
       " 'Let me check for you.',\n",
       " 'Yes, they played against Sporting KC and the game was tied  one - one .',\n",
       " 'Okay, how did they do in the last game?',\n",
       " 'Last Saturday was their last game.',\n",
       " 'Okay and how many games back from first place are they?',\n",
       " 'Real Salt Lake is back  one  zero  games from first place.',\n",
       " 'Okay, thank you.',\n",
       " \"You're welcome, have a great evening.\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utterance_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "stuffed-correlation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_map(s):\n",
    "    # I think using ord in range\n",
    "    if (ord(s) == 32) or ((ord(s) >= 97) and (ord(s)<=122)):\n",
    "        return s\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acquired-virtue",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello. I'd like to find a round trip commercial airline flight from San Francisco to Denver.\n",
      "hello id like to find a round trip commercial airline flight from san francisco to denver\n"
     ]
    }
   ],
   "source": [
    "example_string = flight_conversations[0][0]['text']\n",
    "print(example_string)\n",
    "print(''.join(map(corpus_map, example_string.lower())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "textile-olympus",
   "metadata": {},
   "source": [
    "#### 2. Punctuation, Capitalization and Symbols\n",
    "    \n",
    "I will be removing punctuation. I am interested exploring if its spatial ordering can be used, but it seems wrong to include it in the encoding of the corpus.\n",
    "I will say that the best way to incorporate the context and information it provides is to just split into sentences/chunks with punctuation as delimiters.\n",
    "\n",
    "I am conflicted on capitalization, however; an argument for capitalization is proper nouns which are very special by their definition; i.e.\n",
    "mentioning of a sports team should likely be weighted very heavily. In a frequency based approach, though, the effect would be minimal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-wallet",
   "metadata": {},
   "source": [
    "Next, replace any special characters; those which are neither numbers nor lowercase english alphabet. encoding differences; for example, ISO 8859-1 encoding for \"non-breaking space\"  \\xa0 exists in corpus. Can use map built-in to replace unwanted\n",
    "characters with empty string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "honest-cement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello id like to find a round trip commercial airline flight from san francisco to denver hello how can i help you san francisco to denver got it youre really on top of things i like that so what days are you looking to fly hey what else can you say im looking to fly out sometime today the earliest time today and ill be returning in  four  days so i would like to fly out sometime tonight and fly back in the evening in  four  days from im looking to go to denver im flying out of san francisco that sounds good where you looking to go thats right okay we have prices starting at  three  three  seven  that sounds very good i just have two preferences i want a nonstop flight and id like to get an aisle seat okay nonstop and if i heard you correctly did you say you wanted to leave as early as possible and also nile c yes okay you got it so it looks like united airlines leaves at  nine  two  zero  pm that is nonstop the flight duration is  two  hours and  two  eight  minutes and is priced at  three  three  seven  that sounds very good perfect and what time will i be i would like an evening flight on the return flight in  four  days does that include the return flight how does  five  three  zero  am work well the evening works better okay got it  six  five  five  pm that works thats very good alright perfect so that is united airlines as well excellent thank you so much assistant pleasure all mine enjoy your trip to denver have a great night your welcome i think im all set thank you so much'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(list(map(corpus_map, ' '.join(corpus_text[0][0]).lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "taken-venice",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_text = [[''.join(list(map(corpus_map, ' '.join(conv).lower()))) for conv in topic ] for topic in corpus_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "pointed-value",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello id like to find a round trip commercial airline flight from san francisco to denver hello how can i help you san francisco to denver got it youre really on top of things i like that so what days are you looking to fly hey what else can you say im looking to fly out sometime today the earliest time today and ill be returning in  four  days so i would like to fly out sometime tonight and fly back in the evening in  four  days from im looking to go to denver im flying out of san francisco that sounds good where you looking to go thats right okay we have prices starting at  three  three  seven  that sounds very good i just have two preferences i want a nonstop flight and id like to get an aisle seat okay nonstop and if i heard you correctly did you say you wanted to leave as early as possible and also nile c yes okay you got it so it looks like united airlines leaves at  nine  two  zero  pm that is nonstop the flight duration is  two  hours and  two  eight  minutes and is priced at  three  three  seven  that sounds very good perfect and what time will i be i would like an evening flight on the return flight in  four  days does that include the return flight how does  five  three  zero  am work well the evening works better okay got it  six  five  five  pm that works thats very good alright perfect so that is united airlines as well excellent thank you so much assistant pleasure all mine enjoy your trip to denver have a great night your welcome i think im all set thank you so much'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_text[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-employer",
   "metadata": {},
   "source": [
    "#### 3. Whitespace\n",
    "\n",
    "As previously described in the section intro, the mappings may have introduced errant white space, now the last operation\n",
    "is to remove this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "christian-queensland",
   "metadata": {},
   "outputs": [],
   "source": [
    "def many_whitespace_to_one(string):\n",
    "    return ' '.join(filter(lambda x : x != '', string.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "distant-february",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial string is '     lots   of           whitespace   '\n",
      "The string after split, filter, join combo is 'lots of whitespace'\n"
     ]
    }
   ],
   "source": [
    "teststr = '     lots   of           whitespace   '\n",
    "print(f\"The initial string is '{teststr}'\")\n",
    "\n",
    "processed_str = many_whitespace_to_one(teststr)\n",
    "print(f\"The string after split, filter, join combo is '{processed_str}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geological-potential",
   "metadata": {},
   "source": [
    "The conversations are split into lists of individual terms because that is what gensim wants; but also\n",
    "should write the data to a text file, choose to have each conversation in its own line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "devoted-trance",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('conversation_by_line.txt', 'w') as file:\n",
    "    flattened_corpus = [many_whitespace_to_one(conv) for topic in corpus_text for conv in topic]\n",
    "    for conversation_line in flattened_corpus:\n",
    "        file.write(conversation_line+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confused-legislature",
   "metadata": {},
   "source": [
    "\n",
    "Fraction of corpus vs. number of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "convinced-spider",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2784080 words in total in all text based data\n"
     ]
    }
   ],
   "source": [
    "corpus_with_repeats_size = len(''.join(flattened_corpus).split(' '))\n",
    "print(f\"There are {corpus_with_repeats_size} words in total in all text based data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "lined-anime",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'countdict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-1ec6f0300164>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmostcom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcountdict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mcounts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcountdict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mfraction_vs_num_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mfraction_vs_num_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mcorpus_with_repeats_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'countdict' is not defined"
     ]
    }
   ],
   "source": [
    "mostcom = countdict.most_common()\n",
    "counts = np.array(countdict.most_common())[:, 1].astype(int)\n",
    "fraction_vs_num_words = []\n",
    "for i in range(0, 1000):\n",
    "    fraction_vs_num_words.append(sum(counts[:i])/corpus_with_repeats_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innovative-equilibrium",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title('Percentage of all text by word count')\n",
    "plt.plot(fraction_vs_num_words)\n",
    "plt.xlabel('Number of words, starting with most common')\n",
    "plt.ylabel('Fraction of total text')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clinical-equity",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "countdict = Counter(''.join(flattened_corpus).split(' '))\n",
    "countdict.most_common()[::-1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "studied-controversy",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(np.log10(list(countdict.values())), bins=10)\n",
    "plt.xlabel('log10(frequency)')\n",
    "plt.ylabel('Number of words with this frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refined-definition",
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_corpus = [many_whitespace_to_one(conv).split() for topic in corpus_text for conv in topic]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-mailing",
   "metadata": {},
   "source": [
    "Finally, can write this to a file to save and not have to repeat calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perceived-national",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec, Doc2Vec\n",
    "model = Word2Vec(sentences=common_texts, size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "english-right",
   "metadata": {},
   "source": [
    "Already a fork in the road; can choose to load in the utterances (sentence) individually or by conversation (i.e. paragraph == sentence). For a high level approach I think the latter is more viable, but this requires reading.\n",
    "\n",
    "#### Reason 1: Preliminary investigation is a coarse grained approach to test viability.\n",
    "Only trying to guess the topic, of which there are 7. \n",
    "\n",
    "#### Reason 2: short utterances\n",
    "I still need to read up on word embeddings; I know they are not one-hot encodings, but if they have similar properties then\n",
    "due to the curse of dimensionality, embeddings of 1 word utterances will be very far from median distance even if they correspond to high frequency words. I am imagining a vector with only 1 non-zero entry, but if word2vec chooses a smart basis\n",
    "this won't be the case. \n",
    "\n",
    "#### Reason 3: Performance?\n",
    "I do not know how performance scales with length of sentences vs. number of sentences, again this comes down to the individual\n",
    "algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-munich",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-grenada",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
